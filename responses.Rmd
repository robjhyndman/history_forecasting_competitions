---
title: Response to reviewers
author: INTFOR_1812597
numbersections: no
output:
  MonashEBSTemplates::memo:
    includes:
      in_header: preamble2.tex
fontfamily: mathpazo
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  cache=TRUE,
  messages=FALSE,
  warning=FALSE)
```

Thanks to the editor and the two reviewers for some very helpful comments which have led to many improvements of the paper. My response are in black, the reviewers' comments are in red.

# Associate Editor's comments:

> This is a very well-written paper. The two reviewers give some constructive suggestions for improving the manuscript. I especially enjoyed the quotes and anecdotes. I also have two comments - one minor and one more major:
>
 - P4L49: I would say "The top performing entry was ... "

Done

> - Why not briefly touching on forecasting competitions other than time series? M2 should be briefly mentioned, I think. Also, I would add a small paragraph on the Tetlock-Superforecasters project in section 3.

M2 is now briefly mentioned. To cover other forecasting competitions would make the paper too long and unfocussed.

# Reviewer 1 comments

> P1L37. I would change "Young researchers in forecasting" to "Researchers new to forecasting" or something not about age.

Done

>P2L36-40. While I disagree with Jenkins' original statement, I wouldn't completely dismiss it as "untrue" and go to the other extreme. I believe building a forecasting model is a systems engineering process. Human brain has to be part of the process. I would say "model building is best done by the human brain and computers together and is inevitably an iterative process."

My sentence including the word "untrue" refers to the previous sentence, not Jenkins' statement. The qualifications I provided (enough data and a flexible model) are important. I do not disagree with the reviewers' final sentence above.

> P3L19. Not sure if "insults" is the right word there. Maybe changing it to "questioning the competency of the analyst". I personally believe that the competency of the analyst does affect the forecast accuracy significantly.

Done

> P4L24. Please elaborate "the second was not strictly time series forecasting."

A short description of the M2 competition has been added.

> P4L39. Please explain why the DSES method was not included. If it was not in a published literature, can you ask Spyros and put the answer in this paper? Otherwise, you are leaving the readers wondering why not include DSES.

After checking with Spyros, it seems that DSES was included under another name. I've updated the paper accordingly.

> A general comment to Section 2: somewhere in this section, either beginning or end, it's better mention your definition of "time series" competition. Did you mean to say that explanatory variables were not provided?

A definition has been added to the first paragraph of Section 1. I am including explanatory variables, hence the coverage of the GEFCom competitions.

> P6L39. I don't think "consensus" is the right word there. Many researchers are still advocating ANN and those blackbox models for time series forecasting.

Changed to "consensus in the forecasting community" to mean people who publish in forecasting journals, members of the IIF, etc.

> P6L49. I think it's better characterize Kaggle platform for "data mining" competitions rather than "prediction" competitions.

Done

> P7L30-32. I don't think the info presented there is up to date. I believe Kaggle later changed it's way to calculate the leaderboard. For GEFCom2012, the public leaderboard was based on one subset of the test data, while the final leaderboard was based on a different subset of the test data. There is no overlap between the two subsets.

Fixed

> P7L45. "three" --> "five".

Done

> P7L56. The statement is true for the wind power forecasting track. The load forecasting track asked the contestants to backcast a few sections of missing data "and" to forecast one week after the historical period.

Fixed

> P8L13-17. The qualifying match of GEFCom2017 is about ISONE data. The final match involves 183 delivery point meters. See the intro paper I submitted in Dec.

Fixed

> A general comment to Section 4: Maybe you can add a paragraph or a few sentences to comment on how to disseminate the competition results and findings. such as the importance of formal publications after the competitions. There were many other forecasting competitions in the past, but the ones that are really helpful to the research community are the ones organized with academic journals that can publish findings, data and insights afterwards.

A paragraph along these lines has now been added.

# Referee 2 comments

> A reviewer always has to face up to whether the author should be left to write his or her own paper, or should be required to write the reviewer’s paper for him. Having writing a somewhat similar paper 15 years ago, I obviously am glad to see a new interpretation of the questions arising from forecasting competitions. But there are some important things in this paper I think are missing, in particular there are two issues I would like to see much more fully discussed – the objectives of the different competitions and how they have changed (if at all) and the criticisms levelled. The latter issue is regularly visited in the paper but not I think fully faced up to. At the time of the Fildes & Ord paper the commentary on the M3 competition was not available. The section on p.5 picks up on 5 points focussing more on extensions. However, a quick review of the M3 discussion reveals a number of additional and I would say important points. This links to the lack of clarity as to the objectives of the different competitions. Granger and Newbold were clear (even if misguided) but many of the later
competitions have been opaque.

The objectives of the competitions are largely unstated, and probably confused. I have now added a paragraph about this.

As to covering the criticisms of the M3 competition, it is not clear to me what the reviewer wants me to cover that is not already said. I spent about half a page on criticisms, and included the issues I thought were most important. What have I missed?

> Many of the objections to the competitions related to a lack of a clear population from which the series are drawn. As I commented with Spyros in 1995,
> > *This criticism is valid, but the emphasis is misplaced. “Many (if not all) field experiments suffer from the same limitation of using a non-random sample experimental base. It is overcome by the experimenter increasing the sample size and the diversity of the set (of selected time series) and by ensuring that the non-random components in the experiment are of little importance to the possible outcome. As Johnstone (1989) argues, the statistical testing of hypotheses is not dependent on randomness in the sample, only lack of systematic bias. Inexplicable findings should lead to a revised definition of the population under study and a revised view of the results expected.”*

> I would therefore see the discussion on significance p.5 as perpetuating a misunderstanding. It also suggests that more series (111, 1001, 3000,100K - M4) is better. Well it depends doesn’t it?

I disagree. The biggest issue is that any statistical tests assume a well-defined population. Otherwise inference makes no sense at all. The size of the sample does not resolve that problem. Nor does increasing the diversity of the data set.

I also diagree with Johnstone. Randomness is crucial if statistical testing is done.

> Other issues are the survey of past competitions. Again the objectives need consideration. I note that Fildes and Ord included Meese, R. A., & Geweke, J. (1984). A comparison of autoregressive univariate forecasting procedures for macroeconomic time series *Journal of Business and Economic Statistics*, 2 191-200.

After 1980, I have restricted my attention to true competitions, involving multiple entrants. That rules out Meese and Geweke (1984) and hundreds of other papers since then. I have now made this explicit.

> Also there are competitions which are not fully focussed on univariate time series sata but have relevance, e.g. Ma, S., Fildes, R., & Huang, T. (2016). Demand forecasting with high dimensional data: The case of SKU retail sales forecasting with intra- and inter-category promotional information. *European Journal of Operational Research*, 249, 245-257. This covers 926 SKUs and in passing shows the inadequacy of ETS compared with a causal models. I don’t think there’s a lot of multivariate comparisons out there (Hyndman’s own of course) but this could be used as an example rather than requiring a full survey. We have a paper which is barely passed the conference presentation stage, again on promotional data but it includes all the univariate method as well as extensions to include promotional indicators. TBATS wins with promotional data included.

As noted above, I am restricting attention to true competitions involving multiple participants, not comparisons of methods where the analysis is done by the authors.

> The discussion on replication could be usefully expanded to include papers published in the IJF, e.g. Boylan.

Done

> The final point of what I appreciate is a rather rambling review is that in the discussion of the response to the competitions the Fildes and Makridakis paper shows that the theoretical time series literature up to 1994 did not respond. I haven’t looked again at this issue apart from a superficial check now but I don’t think there’s any evidence of the finding being taken on board by for example time series statisticians.

I do not understand this comment which seems wilfully ignorant of the work done in the last 25 years by me and others. I was a time series statistician before I was a forecaster. To take one example, I read the competition papers which claimed that ARIMA models did not forecast well, so I developed the `auto.arima` algorithm (Hyndman & Khandakar JSS 2008) which does forecast well. Surely that is a response by a time series statistician.

Others have also responded including econometricians such as Diebold. The importance of good forecasting is also now well-recognized in the statistics and econometrics literature. While there are still large parts of the time series community who are obsessed about properties of models, and there continues to be a bias against forecasting papers in the top statistics and econometrics journals, it is not true to say there has been no response.

There has also been substantial theoretical work done on forecast evaluation, partly driven by forecasting competitions. See any of the many Gneiting papers for example.

There has also been theoretical work done on training time series models in order to obtain good forecasts, rather than to maximize the likelihood for example. See the recent IJF paper by Martin, Frazier and Manesthoorn for an example.

> To make my suggestions for revision more focused, I suggest a table that covers objectives and a table/ list that covers methodological changes seen. In addition the various points listed above need
further thought and discussion.

Competition objectives are often unstated, and competition features are too varied to fit into a tabular format. So I have chosen not to do this.